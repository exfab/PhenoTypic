{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding the Image Class: Data Components and Accessors\n",
        "\n",
        "This tutorial provides a comprehensive guide to the PhenoTypic `Image` class architecture, focusing on its main data components and how they interact with detection and analysis modules.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this tutorial, you will understand:\n",
        "- The Image class hierarchy and accessor pattern\n",
        "- The six main data components: `rgb`, `gray`, `enh_gray`, `objmask`, `objmap`, and `metadata`\n",
        "- How these components interact during image processing workflows\n",
        "- When and how to use each component in your analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction: Image Class Architecture\n",
        "\n",
        "### Class Hierarchy\n",
        "\n",
        "The `Image` class inherits from several specialized classes, each adding specific functionality:\n",
        "\n",
        "```\n",
        "Image\n",
        "  â†“\n",
        "ImageIOHandler (file I/O operations)\n",
        "  â†“\n",
        "ImageColorSpace (color space conversions)\n",
        "  â†“\n",
        "...(additional specialized handlers)\n",
        "  â†“\n",
        "ImageHandler (accessor initialization and image operations)\n",
        "  â†“\n",
        "ImageDataManager (core data storage and management)\n",
        "```\n",
        "\n",
        "### The Accessor Pattern\n",
        "\n",
        "PhenoTypic uses an **accessor pattern** to provide intuitive, property-based access to image data. Instead of directly accessing internal arrays, you use accessor objects that:\n",
        "- Provide consistent interfaces across different data types\n",
        "- Handle synchronization between related data (e.g., mask and map)\n",
        "- Support NumPy-style indexing with bracket notation: `image.rgb[:]`\n",
        "- Enable method chaining and specialized operations\n",
        "\n",
        "### Internal Data Storage\n",
        "\n",
        "The Image class stores data in two main containers:\n",
        "\n",
        "1. **`_data` (ImageData)**: Holds the actual image arrays\n",
        "   - `rgb`: NumPy array (uint8/uint16) for color images\n",
        "   - `gray`: NumPy array (float32) for grayscale representation\n",
        "   - `enh_gray`: NumPy array (float32) for enhanced grayscale\n",
        "   - `sparse_object_map`: Sparse CSC matrix (uint16) for labeled objects\n",
        "\n",
        "2. **`_metadata` (Metadata)**: Stores image information in three tiers\n",
        "   - `private`: Internal use only (e.g., UUID)\n",
        "   - `protected`: System-managed (e.g., name, bit_depth, image_type)\n",
        "   - `public`: User-modifiable custom metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import phenotypic as pht\n",
        "from phenotypic.detect import OtsuDetector\n",
        "\n",
        "# Set matplotlib style for better visualization\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['figure.dpi'] = 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Loading an Image and Basic Properties\n",
        "\n",
        "Let's start by loading a sample plate image from the built-in dataset. We'll use an image of bacterial colonies growing on agar media.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a sample plate image (12 hours of growth)\n",
        "image = pht.data.load_plate_12hr(mode='Image')\n",
        "\n",
        "# Display basic properties\n",
        "print(f\"Image name: {image.name}\")\n",
        "print(f\"Image shape: {image.shape}\")\n",
        "print(f\"Bit depth: {image.bit_depth}\")\n",
        "print(f\"Data type: {type(image)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the original image\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "image.show(ax=ax)\n",
        "ax.set_title(f'{image.name} - Original Image', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. RGB Component: Multichannel Color Data\n",
        "\n",
        "The **`rgb`** accessor provides access to the multichannel RGB representation of the image.\n",
        "\n",
        "### Key Characteristics:\n",
        "- **Data type**: uint8 (for 8-bit) or uint16 (for 16-bit images)\n",
        "- **Shape**: `(height, width, 3)` - three channels for Red, Green, Blue\n",
        "- **Access pattern**: Use bracket notation `image.rgb[:]` to get the array\n",
        "- **Automatically set**: When loading a color image\n",
        "- **Empty for grayscale**: If you load a 2D array, RGB remains empty\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Access the RGB data\n",
        "rgb_data = image.rgb[:]\n",
        "\n",
        "print(f\"RGB shape: {rgb_data.shape}\")\n",
        "print(f\"RGB dtype: {rgb_data.dtype}\")\n",
        "print(f\"RGB value range: [{rgb_data.min()}, {rgb_data.max()}]\")\n",
        "print(f\"\\nIs RGB empty? {image.rgb.isempty()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize RGB and individual channels\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
        "\n",
        "# Full RGB\n",
        "axes[0, 0].imshow(rgb_data)\n",
        "axes[0, 0].set_title('RGB (Full Color)', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "# Red channel\n",
        "axes[0, 1].imshow(rgb_data[:, :, 0], cmap='Reds')\n",
        "axes[0, 1].set_title('Red Channel', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].axis('off')\n",
        "\n",
        "# Green channel\n",
        "axes[1, 0].imshow(rgb_data[:, :, 1], cmap='Greens')\n",
        "axes[1, 0].set_title('Green Channel', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].axis('off')\n",
        "\n",
        "# Blue channel\n",
        "axes[1, 1].imshow(rgb_data[:, :, 2], cmap='Blues')\n",
        "axes[1, 1].set_title('Blue Channel', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Gray Component: Grayscale Representation\n",
        "\n",
        "The **`gray`** accessor provides the grayscale representation of the image using weighted luminance conversion.\n",
        "\n",
        "### Key Characteristics:\n",
        "- **Data type**: float32 with values in [0, 1] range\n",
        "- **Shape**: `(height, width)` - single channel\n",
        "- **Conversion**: Automatically created from RGB using `skimage.color.rgb2gray`\n",
        "- **Weighted formula**: `0.2125*R + 0.7154*G + 0.0721*B` (ITU-R BT.709)\n",
        "- **Read-only**: Changes to gray won't modify RGB (use `enh_gray` for modifications)\n",
        "- **Always present**: Even for grayscale-loaded images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Access the grayscale data\n",
        "gray_data = image.gray[:]\n",
        "\n",
        "print(f\"Gray shape: {gray_data.shape}\")\n",
        "print(f\"Gray dtype: {gray_data.dtype}\")\n",
        "print(f\"Gray value range: [{gray_data.min():.4f}, {gray_data.max():.4f}]\")\n",
        "print(f\"\\nIs gray empty? {image.gray.isempty()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize grayscale conversion\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Original RGB\n",
        "axes[0].imshow(rgb_data)\n",
        "axes[0].set_title('Original RGB Image', fontsize=12, fontweight='bold')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Grayscale\n",
        "axes[1].imshow(gray_data, cmap='gray')\n",
        "axes[1].set_title('Grayscale (Weighted Luminance)', fontsize=12, fontweight='bold')\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nğŸ’¡ Note: The grayscale conversion preserves perceptual brightness better than simple averaging.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Enhanced Gray Component: Mutable Processing Copy\n",
        "\n",
        "The **`enh_gray`** accessor provides a mutable copy of the grayscale image for preprocessing and enhancement.\n",
        "\n",
        "### Key Characteristics:\n",
        "- **Data type**: float32 with values in [0, 1] range\n",
        "- **Shape**: `(height, width)` - single channel\n",
        "- **Initialization**: Starts as an exact copy of `gray`\n",
        "- **Mutable**: Can be modified without affecting the original `gray`\n",
        "- **Primary use**: Detection algorithms process `enh_gray`, preserving original data\n",
        "- **Workflow**: Apply enhancement operations â†’ Detector uses `enh_gray` â†’ Original stays intact\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Access enhanced grayscale data\n",
        "enh_gray_data = image.enh_gray[:]\n",
        "\n",
        "print(f\"Enhanced gray shape: {enh_gray_data.shape}\")\n",
        "print(f\"Enhanced gray dtype: {enh_gray_data.dtype}\")\n",
        "print(f\"Enhanced gray value range: [{enh_gray_data.min():.4f}, {enh_gray_data.max():.4f}]\")\n",
        "\n",
        "# Check if enh_gray starts as a copy of gray\n",
        "print(f\"\\nAre gray and enh_gray initially equal? {np.allclose(gray_data, enh_gray_data)}\")\n",
        "print(f\"Are they the same object? {np.shares_memory(gray_data, enh_gray_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate that enh_gray can be modified independently\n",
        "# Let's apply a simple enhancement (histogram equalization)\n",
        "from skimage.exposure import equalize_adapthist\n",
        "\n",
        "# Create a copy to show the enhancement\n",
        "enhanced_demo = equalize_adapthist(enh_gray_data, clip_limit=0.03)\n",
        "\n",
        "# Visualize the difference\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "axes[0].imshow(gray_data, cmap='gray')\n",
        "axes[0].set_title('Gray (Original)', fontsize=12, fontweight='bold')\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(enh_gray_data, cmap='gray')\n",
        "axes[1].set_title('Enh_Gray (Initial Copy)', fontsize=12, fontweight='bold')\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(enhanced_demo, cmap='gray')\n",
        "axes[2].set_title('Enh_Gray (After CLAHE)', fontsize=12, fontweight='bold')\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nğŸ’¡ Note: Enhancement operations can be applied to enh_gray without affecting the original gray data.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Object Mask and Object Map: Detection Results\n",
        "\n",
        "The **`objmask`** and **`objmap`** accessors store the results of object detection algorithms. Let's apply the `OtsuDetector` to see them in action.\n",
        "\n",
        "### OtsuDetector Workflow:\n",
        "1. Reads `enh_gray` data\n",
        "2. Calculates optimal threshold using Otsu's method\n",
        "3. Creates binary mask and stores in `objmask`\n",
        "4. Labels connected regions and stores in `objmap`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply OtsuDetector to detect colonies\n",
        "detector = OtsuDetector(ignore_zeros=True, ignore_borders=True)\n",
        "detector.apply(image)\n",
        "\n",
        "print(\"âœ… Detection complete!\")\n",
        "print(f\"Number of detected objects: {image.num_objects}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Object Mask: Binary Representation\n",
        "\n",
        "The **`objmask`** is a binary mask showing which pixels belong to detected objects.\n",
        "\n",
        "### Key Characteristics:\n",
        "- **Values**: 0 (background) and 1 (object pixels)\n",
        "- **Shape**: `(height, width)` - matches image dimensions\n",
        "- **Storage**: Backed by sparse CSC matrix for memory efficiency\n",
        "- **Mutable**: Can be modified (triggers automatic objmap relabeling)\n",
        "- **Set by**: Detection algorithms (Otsu, Watershed, etc.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Access object mask\n",
        "objmask_data = image.objmask[:]\n",
        "\n",
        "print(f\"Object mask shape: {objmask_data.shape}\")\n",
        "print(f\"Object mask dtype: {objmask_data.dtype}\")\n",
        "print(f\"Unique values: {np.unique(objmask_data)}\")\n",
        "print(f\"Number of object pixels: {np.sum(objmask_data)}\")\n",
        "print(f\"Number of background pixels: {np.sum(objmask_data == 0)}\")\n",
        "print(f\"Percentage of image covered by objects: {100 * np.sum(objmask_data) / objmask_data.size:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the object mask\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "# Original grayscale\n",
        "axes[0].imshow(gray_data, cmap='gray')\n",
        "axes[0].set_title('Original Grayscale', fontsize=12, fontweight='bold')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Binary mask\n",
        "axes[1].imshow(objmask_data, cmap='binary')\n",
        "axes[1].set_title('Object Mask (Binary)', fontsize=12, fontweight='bold')\n",
        "axes[1].axis('off')\n",
        "\n",
        "# Overlay on original\n",
        "axes[2].imshow(gray_data, cmap='gray')\n",
        "axes[2].imshow(objmask_data, cmap='Reds', alpha=0.4)\n",
        "axes[2].set_title('Mask Overlay on Image', fontsize=12, fontweight='bold')\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nğŸ’¡ Note: The binary mask shows all detected object pixels without distinguishing individual objects.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Object Map: Labeled Regions\n",
        "\n",
        "The **`objmap`** assigns a unique integer ID to each connected region in the object mask.\n",
        "\n",
        "### Key Characteristics:\n",
        "- **Values**: 0 (background) and unique integers for each object (1, 2, 3, ...)\n",
        "- **Shape**: `(height, width)` - matches image dimensions\n",
        "- **Storage**: Sparse CSC matrix (shared backend with objmask)\n",
        "- **Automatic labeling**: Created/updated when objmask changes\n",
        "- **Used for**: Individual object analysis, measurements, slicing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Access object map\n",
        "objmap_data = image.objmap[:]\n",
        "\n",
        "print(f\"Object map shape: {objmap_data.shape}\")\n",
        "print(f\"Object map dtype: {objmap_data.dtype}\")\n",
        "print(f\"Number of unique objects: {len(np.unique(objmap_data)) - 1}\")\n",
        "print(f\"Object IDs range: {objmap_data[objmap_data > 0].min()} to {objmap_data.max()}\")\n",
        "print(f\"\\nFirst 10 object IDs: {sorted(np.unique(objmap_data[objmap_data > 0]))[:10]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the labeled object map\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "# Object mask (binary)\n",
        "axes[0].imshow(objmask_data, cmap='binary')\n",
        "axes[0].set_title(f'Object Mask\\n(Binary: {image.num_objects} objects)', fontsize=12, fontweight='bold')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Object map (labeled)\n",
        "axes[1].imshow(objmap_data, cmap='nipy_spectral')\n",
        "axes[1].set_title('Object Map\\n(Each color = unique object)', fontsize=12, fontweight='bold')\n",
        "axes[1].axis('off')\n",
        "\n",
        "# Overlay on original\n",
        "axes[2].imshow(rgb_data)\n",
        "# Create a masked array to show only objects\n",
        "masked_objmap = np.ma.masked_where(objmap_data == 0, objmap_data)\n",
        "axes[2].imshow(masked_objmap, cmap='nipy_spectral', alpha=0.5, interpolation='none')\n",
        "axes[2].set_title('Labeled Objects Overlay', fontsize=12, fontweight='bold')\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nğŸ’¡ Note: Each color in the object map represents a distinct detected colony.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Relationship Between objmask and objmap\n",
        "\n",
        "The objmask and objmap are tightly coupled:\n",
        "- **Shared backend**: Both use the same sparse CSC matrix storage\n",
        "- **objmask â†’ objmap**: Binary mask shows \"where objects are\"\n",
        "- **objmap â†’ objmask**: Labeled map shows \"which object each pixel belongs to\"\n",
        "- **Automatic sync**: Modifying objmask triggers relabeling of objmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate the relationship\n",
        "print(\"Relationship verification:\")\n",
        "print(f\"objmask non-zero pixels: {np.count_nonzero(objmask_data)}\")\n",
        "print(f\"objmap non-zero pixels: {np.count_nonzero(objmap_data)}\")\n",
        "print(f\"Are non-zero locations identical? {np.all((objmask_data > 0) == (objmap_data > 0))}\")\n",
        "\n",
        "print(f\"\\nobjmask values: {np.unique(objmask_data)}\")\n",
        "print(f\"objmap values: {len(np.unique(objmap_data))} unique (0 + {image.num_objects} objects)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Metadata: Image Information Storage\n",
        "\n",
        "The **`metadata`** accessor provides a three-tier system for storing image information.\n",
        "\n",
        "### Three-Tier System:\n",
        "\n",
        "1. **Private**: Internal system use only\n",
        "   - UUID for unique identification\n",
        "   - Not directly modifiable by users\n",
        "\n",
        "2. **Protected**: System-managed metadata\n",
        "   - `image_name`: Image identifier\n",
        "   - `bit_depth`: 8 or 16 bit\n",
        "   - `image_type`: BASE, OBJECT, or GRID\n",
        "   - Can be read but modification is restricted\n",
        "\n",
        "3. **Public**: User-modifiable metadata\n",
        "   - Custom key-value pairs\n",
        "   - Experimental conditions, timestamps, etc.\n",
        "   - Fully modifiable by users\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Access metadata\n",
        "print(\"=\" * 50)\n",
        "print(\"METADATA CONTENTS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"\\nAll available metadata keys:\")\n",
        "for key in image.metadata.keys():\n",
        "    print(f\"  - {key}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 50)\n",
        "print(\"Key metadata values:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Name: {image.metadata['image_name']}\")\n",
        "print(f\"Bit depth: {image.metadata['bit_depth']}\")\n",
        "print(f\"Image type: {image.metadata['image_type']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add custom metadata (public tier)\n",
        "image.metadata['experiment_id'] = 'EXP_001'\n",
        "image.metadata['growth_time_hours'] = 12\n",
        "image.metadata['strain'] = 'E. coli K-12'\n",
        "image.metadata['temperature_celsius'] = 37.0\n",
        "image.metadata['media_type'] = 'LB Agar'\n",
        "\n",
        "print(\"âœ… Custom metadata added!\\n\")\n",
        "\n",
        "# Display all metadata\n",
        "print(\"Updated metadata:\")\n",
        "for key, value in image.metadata.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Component Interactions: Complete Workflow\n",
        "\n",
        "Let's visualize how all components work together in a typical image processing workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a comprehensive visualization showing all components\n",
        "fig = plt.figure(figsize=(16, 12))\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# Row 1: Source data\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "ax1.imshow(image.rgb[:])\n",
        "ax1.set_title('1. RGB Component\\n(Color image data)', fontsize=11, fontweight='bold')\n",
        "ax1.axis('off')\n",
        "\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "ax2.imshow(image.gray[:], cmap='gray')\n",
        "ax2.set_title('2. Gray Component\\n(Luminance conversion)', fontsize=11, fontweight='bold')\n",
        "ax2.axis('off')\n",
        "\n",
        "ax3 = fig.add_subplot(gs[0, 2])\n",
        "ax3.imshow(image.enh_gray[:], cmap='gray')\n",
        "ax3.set_title('3. Enh_Gray Component\\n(Processing copy)', fontsize=11, fontweight='bold')\n",
        "ax3.axis('off')\n",
        "\n",
        "# Row 2: Detection results\n",
        "ax4 = fig.add_subplot(gs[1, 0])\n",
        "ax4.imshow(image.objmask[:], cmap='binary')\n",
        "ax4.set_title(f'4. ObjMask Component\\n(Binary mask: {image.num_objects} objects)', fontsize=11, fontweight='bold')\n",
        "ax4.axis('off')\n",
        "\n",
        "ax5 = fig.add_subplot(gs[1, 1])\n",
        "ax5.imshow(image.objmap[:], cmap='nipy_spectral')\n",
        "ax5.set_title('5. ObjMap Component\\n(Labeled regions)', fontsize=11, fontweight='bold')\n",
        "ax5.axis('off')\n",
        "\n",
        "ax6 = fig.add_subplot(gs[1, 2])\n",
        "# Create text display for metadata\n",
        "ax6.axis('off')\n",
        "metadata_text = \"6. Metadata Component\\n\" + \"=\"*25 + \"\\n\"\n",
        "metadata_text += f\"Name: {image.metadata.get('image_name', 'N/A')}\\n\"\n",
        "metadata_text += f\"Bit depth: {image.metadata.get('bit_depth', 'N/A')}\\n\"\n",
        "metadata_text += f\"Type: {image.metadata.get('image_type', 'N/A')}\\n\"\n",
        "metadata_text += f\"Experiment: {image.metadata.get('experiment_id', 'N/A')}\\n\"\n",
        "metadata_text += f\"Strain: {image.metadata.get('strain', 'N/A')}\\n\"\n",
        "metadata_text += f\"Growth time: {image.metadata.get('growth_time_hours', 'N/A')} hrs\\n\"\n",
        "ax6.text(0.1, 0.5, metadata_text, fontsize=10, family='monospace',\n",
        "         verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "ax6.set_title('Metadata', fontsize=11, fontweight='bold')\n",
        "\n",
        "# Row 3: Overlays\n",
        "ax7 = fig.add_subplot(gs[2, :])\n",
        "ax7.imshow(image.rgb[:])\n",
        "masked_objmap = np.ma.masked_where(image.objmap[:] == 0, image.objmap[:])\n",
        "ax7.imshow(masked_objmap, cmap='nipy_spectral', alpha=0.5, interpolation='none')\n",
        "ax7.set_title(f'Complete Workflow Result: {image.num_objects} Detected Colonies', \n",
        "              fontsize=12, fontweight='bold')\n",
        "ax7.axis('off')\n",
        "\n",
        "plt.suptitle('PhenoTypic Image Class: All Data Components', fontsize=16, fontweight='bold', y=0.98)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Workflow Summary\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ 1. Load RGB Image (from file or array)                         â”‚\n",
        "â”‚    â†’ image = pht.Image.imread('plate.jpg')                      â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                     â†“\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ 2. Auto-convert to Gray (weighted luminance)                   â”‚\n",
        "â”‚    â†’ image.gray[:] available immediately                        â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                     â†“\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ 3. Initialize Enh_Gray (copy of gray)                          â”‚\n",
        "â”‚    â†’ image.enh_gray[:] = image.gray[:].copy()                  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                     â†“\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ 4. Apply Detector (operates on enh_gray)                       â”‚\n",
        "â”‚    â†’ detector.apply(image)                                      â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                     â†“\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ 5. Detector Sets ObjMask (binary detection result)             â”‚\n",
        "â”‚    â†’ image.objmask[:] = binary_mask                             â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                     â†“\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ 6. Auto-label ObjMap (connected component labeling)            â”‚\n",
        "â”‚    â†’ image.objmap[:] automatically updated                      â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Practical Tips and Best Practices\n",
        "\n",
        "### When to Use Each Component\n",
        "\n",
        "| Component | Use When You Need To... |\n",
        "|-----------|------------------------|\n",
        "| **rgb** | Access color information, display full-color images, analyze individual color channels |\n",
        "| **gray** | Work with intensity data, maintain original data integrity, perform measurements |\n",
        "| **enh_gray** | Apply preprocessing, test enhancement algorithms, prepare for detection |\n",
        "| **objmask** | Work with binary masks, perform morphological operations, modify detection results |\n",
        "| **objmap** | Analyze individual objects, measure per-object properties, extract specific colonies |\n",
        "| **metadata** | Store experimental context, track processing parameters, organize datasets |\n",
        "\n",
        "### Accessor Pattern Benefits\n",
        "\n",
        "1. **Consistent Interface**: All accessors use bracket notation `[:]`\n",
        "2. **Data Integrity**: Read-only accessors prevent accidental modifications\n",
        "3. **Automatic Sync**: Changes to objmask automatically update objmap\n",
        "4. **Memory Efficiency**: Sparse storage for object maps saves memory\n",
        "5. **Type Safety**: Accessors enforce correct data types\n",
        "\n",
        "### Read-Only vs Mutable Components\n",
        "\n",
        "**Read-Only** (viewing via `[:]` returns non-writable views):\n",
        "- `rgb`: Preserve color data integrity\n",
        "- `gray`: Maintain original grayscale reference\n",
        "\n",
        "**Mutable** (can be modified in-place):\n",
        "- `enh_gray`: Designed for preprocessing\n",
        "- `objmask`: Can be refined post-detection\n",
        "- `objmap`: Automatically updates with objmask\n",
        "- `metadata`: Fully modifiable for custom data\n",
        "\n",
        "### Integration with Other PhenoTypic Modules\n",
        "\n",
        "**Detection** (`phenotypic.detect`):\n",
        "```python\n",
        "from phenotypic.detect import OtsuDetector, WatershedDetector\n",
        "detector = OtsuDetector()\n",
        "detector.apply(image)  # Populates objmask and objmap\n",
        "```\n",
        "\n",
        "**Enhancement** (`phenotypic.enhance`):\n",
        "```python\n",
        "from phenotypic.enhance import CLAHE, GaussianBlur\n",
        "enhancer = CLAHE()\n",
        "enhancer.apply(image)  # Modifies enh_gray\n",
        "```\n",
        "\n",
        "**Measurement** (`phenotypic.measure`):\n",
        "```python\n",
        "from phenotypic.measure import MeasureSize\n",
        "measurer = MeasureSize()\n",
        "results = measurer.apply(image)  # Uses objmap for per-object analysis\n",
        "```\n",
        "\n",
        "**Analysis** (`phenotypic.analysis`):\n",
        "```python\n",
        "# Access individual objects using objmap labels\n",
        "for obj in image.objects:\n",
        "    print(f\"Object {obj.label}: Area = {obj.area}\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### Architecture\n",
        "âœ… The Image class uses a layered inheritance structure with specialized handlers\n",
        "\n",
        "âœ… The accessor pattern provides intuitive, property-based access to image data\n",
        "\n",
        "âœ… Internal storage uses `_data` (ImageData) and `_metadata` (Metadata) containers\n",
        "\n",
        "### Data Components\n",
        "âœ… **rgb**: Color image data (uint8/uint16, 3 channels)\n",
        "\n",
        "âœ… **gray**: Grayscale representation (float32, auto-converted via weighted luminance)\n",
        "\n",
        "âœ… **enh_gray**: Mutable processing copy (float32, for preprocessing/detection)\n",
        "\n",
        "âœ… **objmask**: Binary object mask (0/1, shows detected pixels)\n",
        "\n",
        "âœ… **objmap**: Labeled object regions (0/1/2/3..., unique ID per object)\n",
        "\n",
        "âœ… **metadata**: Three-tier information storage (private/protected/public)\n",
        "\n",
        "### Workflow\n",
        "âœ… Loading â†’ Gray conversion â†’ Enh_gray initialization â†’ Detection â†’ Mask creation â†’ Map labeling\n",
        "\n",
        "âœ… Components are synchronized automatically (objmask â†” objmap)\n",
        "\n",
        "âœ… Original data remains intact during processing (gray vs enh_gray separation)\n",
        "\n",
        "### Best Practices\n",
        "âœ… Use bracket notation `[:]` to access array data from accessors\n",
        "\n",
        "âœ… Modify `enh_gray` for preprocessing, keep `gray` as reference\n",
        "\n",
        "âœ… Store experimental metadata in the public tier for reproducibility\n",
        "\n",
        "âœ… Use objmap for individual object analysis, objmask for morphological operations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Now that you understand the Image class structure, explore:\n",
        "\n",
        "1. **Grid Images**: Specialized subclass for arrayed colonies (`GridImage`)\n",
        "2. **Image Sets**: Batch processing multiple images (`ImageSet`)\n",
        "3. **Detection Algorithms**: Various threshold and watershed methods\n",
        "4. **Enhancement Operations**: CLAHE, filtering, morphological operations\n",
        "5. **Measurement Tools**: Size, intensity, shape, texture analysis\n",
        "6. **Interactive Tools**: Jupyter-based interactive measurement (`image.interactive_measure()`)\n",
        "\n",
        "Check out the other example notebooks for more advanced workflows!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
